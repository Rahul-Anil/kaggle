{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\n",
    "    \"/Users/rahulanil/garchomp/projects/kaggle/titanic/data/train.csv\"\n",
    ")\n",
    "test_df = pd.read_csv(\"/Users/rahulanil/garchomp/projects/kaggle/titanic/data/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import titanic_preprocessing as tp\n",
    "\n",
    "test_df_passengeId = test_df[\"PassengerId\"]\n",
    "train_df, test_df = tp.generic_perprocessing(train_df, test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_df.loc[:, train_df.columns != \"Survived\"]\n",
    "y = train_df[\"Survived\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_ct shpae: (889, 21)\n",
      "X_test shpae: (418, 21)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "\n",
    "# Survived = y\n",
    "# OneHotEncoding: Pclass, Sex, Embarked, Initials\n",
    "# StandardScalar: Age, SibSp, Parch, Fare, Family_size\n",
    "\n",
    "ct = ColumnTransformer(\n",
    "    [\n",
    "        (\n",
    "            \"onehot\",\n",
    "            OneHotEncoder(sparse=False),\n",
    "            [\"Pclass\", \"Sex\", \"Embarked\", \"Initials\"],\n",
    "        ),\n",
    "        (\n",
    "            \"StandardScaler\",\n",
    "            StandardScaler(),\n",
    "            [\"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Family_size\"],\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "ct.fit(X)\n",
    "X_ct = ct.transform(X)\n",
    "print(f\"X_ct shpae: {X_ct.shape}\")\n",
    "\n",
    "test = ct.transform(test_df)\n",
    "print(f\"X_test shpae: {test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clf score: 0.8312710911136107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rahulanil/garchomp/projects/kaggle/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression(penalty=\"none\", C=0.01, max_iter=10000)\n",
    "clf2 = LogisticRegression(penalty=\"l2\", C=0.01, max_iter=10000)\n",
    "clf.fit(X_ct, y)\n",
    "print(f\"clf score: {clf.score(X_ct, y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = [\n",
    "    {\n",
    "        \"penalty\": [\"l2\", \"none\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 2, 3, 4, 5, 10],\n",
    "        \"max_iter\": [10000],\n",
    "        \"n_jobs\": [-1],\n",
    "    },\n",
    "    {\n",
    "        \"penalty\": [\"l1\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 2, 3, 4, 5, 10],\n",
    "        \"max_iter\": [10000],\n",
    "        \"n_jobs\": [-1],\n",
    "        \"solver\": [\"saga\"],\n",
    "    },\n",
    "    {\n",
    "        \"penalty\": [\"elasticnet\"],\n",
    "        \"C\": [0.001, 0.01, 0.1, 1, 2, 3, 4, 5, 10],\n",
    "        \"max_iter\": [10000],\n",
    "        \"n_jobs\": [-1],\n",
    "        \"solver\": [\"saga\"],\n",
    "        \"l1_ratio\": [0.2, 0.4, 0.6, 0.8],\n",
    "    },\n",
    "]\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    LogisticRegression(), param_grid, cv=5, return_train_score=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test scores: 0.7713004484304933\n",
      "Best parametesrs: {'C': 1, 'max_iter': 10000, 'n_jobs': -1, 'penalty': 'l2'}\n",
      "best cross validation score: 0.8468970934799686\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_ct, y, random_state=0)\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"test scores: {grid_search.score(X_test, y_test)}\")\n",
    "print(f\"Best parametesrs: {grid_search.best_params_}\")\n",
    "print(f\"best cross validation score: {grid_search.best_score_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_C</th>\n",
       "      <th>param_max_iter</th>\n",
       "      <th>param_n_jobs</th>\n",
       "      <th>param_penalty</th>\n",
       "      <th>param_solver</th>\n",
       "      <th>param_l1_ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>split3_train_score</th>\n",
       "      <th>split4_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.942032</td>\n",
       "      <td>1.406792</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10000</td>\n",
       "      <td>-1</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.627629</td>\n",
       "      <td>0.005837</td>\n",
       "      <td>56</td>\n",
       "      <td>0.627820</td>\n",
       "      <td>0.628518</td>\n",
       "      <td>0.634146</td>\n",
       "      <td>0.637899</td>\n",
       "      <td>0.628518</td>\n",
       "      <td>0.631380</td>\n",
       "      <td>0.003980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.186262</td>\n",
       "      <td>0.137828</td>\n",
       "      <td>0.000425</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.001</td>\n",
       "      <td>10000</td>\n",
       "      <td>-1</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840893</td>\n",
       "      <td>0.032421</td>\n",
       "      <td>37</td>\n",
       "      <td>0.859023</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.849906</td>\n",
       "      <td>0.870544</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.853606</td>\n",
       "      <td>0.010040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.004507</td>\n",
       "      <td>0.000304</td>\n",
       "      <td>0.000394</td>\n",
       "      <td>0.000039</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10000</td>\n",
       "      <td>-1</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.821344</td>\n",
       "      <td>0.029692</td>\n",
       "      <td>52</td>\n",
       "      <td>0.825188</td>\n",
       "      <td>0.827392</td>\n",
       "      <td>0.825516</td>\n",
       "      <td>0.836773</td>\n",
       "      <td>0.810507</td>\n",
       "      <td>0.825075</td>\n",
       "      <td>0.008422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.013684</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.000342</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.01</td>\n",
       "      <td>10000</td>\n",
       "      <td>-1</td>\n",
       "      <td>none</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.840893</td>\n",
       "      <td>0.032421</td>\n",
       "      <td>37</td>\n",
       "      <td>0.859023</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.849906</td>\n",
       "      <td>0.870544</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.853606</td>\n",
       "      <td>0.010040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.061972</td>\n",
       "      <td>0.114324</td>\n",
       "      <td>0.000338</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.1</td>\n",
       "      <td>10000</td>\n",
       "      <td>-1</td>\n",
       "      <td>l2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836348</td>\n",
       "      <td>0.035332</td>\n",
       "      <td>46</td>\n",
       "      <td>0.832707</td>\n",
       "      <td>0.831144</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.857411</td>\n",
       "      <td>0.825516</td>\n",
       "      <td>0.838586</td>\n",
       "      <td>0.011597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.133081</td>\n",
       "      <td>0.010358</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>0.000029</td>\n",
       "      <td>5</td>\n",
       "      <td>10000</td>\n",
       "      <td>-1</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.843890</td>\n",
       "      <td>0.034169</td>\n",
       "      <td>25</td>\n",
       "      <td>0.859023</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.851782</td>\n",
       "      <td>0.870544</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.853981</td>\n",
       "      <td>0.009929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.111952</td>\n",
       "      <td>0.039497</td>\n",
       "      <td>0.000356</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>10</td>\n",
       "      <td>10000</td>\n",
       "      <td>-1</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845393</td>\n",
       "      <td>0.032179</td>\n",
       "      <td>5</td>\n",
       "      <td>0.859023</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.849906</td>\n",
       "      <td>0.870544</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.853606</td>\n",
       "      <td>0.010040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>0.120149</td>\n",
       "      <td>0.037937</td>\n",
       "      <td>0.000426</td>\n",
       "      <td>0.000152</td>\n",
       "      <td>10</td>\n",
       "      <td>10000</td>\n",
       "      <td>-1</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.4</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845393</td>\n",
       "      <td>0.032179</td>\n",
       "      <td>5</td>\n",
       "      <td>0.859023</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.849906</td>\n",
       "      <td>0.870544</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.853606</td>\n",
       "      <td>0.010040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>0.141615</td>\n",
       "      <td>0.035870</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>10</td>\n",
       "      <td>10000</td>\n",
       "      <td>-1</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.6</td>\n",
       "      <td>...</td>\n",
       "      <td>0.845393</td>\n",
       "      <td>0.032179</td>\n",
       "      <td>5</td>\n",
       "      <td>0.859023</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.849906</td>\n",
       "      <td>0.870544</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.853981</td>\n",
       "      <td>0.009714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>0.167655</td>\n",
       "      <td>0.033794</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>10</td>\n",
       "      <td>10000</td>\n",
       "      <td>-1</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>saga</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842386</td>\n",
       "      <td>0.033037</td>\n",
       "      <td>35</td>\n",
       "      <td>0.859023</td>\n",
       "      <td>0.844278</td>\n",
       "      <td>0.849906</td>\n",
       "      <td>0.870544</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.853981</td>\n",
       "      <td>0.009714</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time param_C  \\\n",
       "0        0.942032      1.406792         0.001515        0.002010   0.001   \n",
       "1        0.186262      0.137828         0.000425        0.000030   0.001   \n",
       "2        0.004507      0.000304         0.000394        0.000039    0.01   \n",
       "3        0.013684      0.002271         0.000342        0.000028    0.01   \n",
       "4        0.061972      0.114324         0.000338        0.000058     0.1   \n",
       "..            ...           ...              ...             ...     ...   \n",
       "58       0.133081      0.010358         0.000366        0.000029       5   \n",
       "59       0.111952      0.039497         0.000356        0.000010      10   \n",
       "60       0.120149      0.037937         0.000426        0.000152      10   \n",
       "61       0.141615      0.035870         0.000372        0.000038      10   \n",
       "62       0.167655      0.033794         0.000445        0.000198      10   \n",
       "\n",
       "   param_max_iter param_n_jobs param_penalty param_solver param_l1_ratio  ...  \\\n",
       "0           10000           -1            l2          NaN            NaN  ...   \n",
       "1           10000           -1          none          NaN            NaN  ...   \n",
       "2           10000           -1            l2          NaN            NaN  ...   \n",
       "3           10000           -1          none          NaN            NaN  ...   \n",
       "4           10000           -1            l2          NaN            NaN  ...   \n",
       "..            ...          ...           ...          ...            ...  ...   \n",
       "58          10000           -1    elasticnet         saga            0.8  ...   \n",
       "59          10000           -1    elasticnet         saga            0.2  ...   \n",
       "60          10000           -1    elasticnet         saga            0.4  ...   \n",
       "61          10000           -1    elasticnet         saga            0.6  ...   \n",
       "62          10000           -1    elasticnet         saga            0.8  ...   \n",
       "\n",
       "   mean_test_score  std_test_score  rank_test_score  split0_train_score  \\\n",
       "0         0.627629        0.005837               56            0.627820   \n",
       "1         0.840893        0.032421               37            0.859023   \n",
       "2         0.821344        0.029692               52            0.825188   \n",
       "3         0.840893        0.032421               37            0.859023   \n",
       "4         0.836348        0.035332               46            0.832707   \n",
       "..             ...             ...              ...                 ...   \n",
       "58        0.843890        0.034169               25            0.859023   \n",
       "59        0.845393        0.032179                5            0.859023   \n",
       "60        0.845393        0.032179                5            0.859023   \n",
       "61        0.845393        0.032179                5            0.859023   \n",
       "62        0.842386        0.033037               35            0.859023   \n",
       "\n",
       "    split1_train_score  split2_train_score  split3_train_score  \\\n",
       "0             0.628518            0.634146            0.637899   \n",
       "1             0.844278            0.849906            0.870544   \n",
       "2             0.827392            0.825516            0.836773   \n",
       "3             0.844278            0.849906            0.870544   \n",
       "4             0.831144            0.846154            0.857411   \n",
       "..                 ...                 ...                 ...   \n",
       "58            0.844278            0.851782            0.870544   \n",
       "59            0.844278            0.849906            0.870544   \n",
       "60            0.844278            0.849906            0.870544   \n",
       "61            0.844278            0.849906            0.870544   \n",
       "62            0.844278            0.849906            0.870544   \n",
       "\n",
       "    split4_train_score  mean_train_score  std_train_score  \n",
       "0             0.628518          0.631380         0.003980  \n",
       "1             0.844278          0.853606         0.010040  \n",
       "2             0.810507          0.825075         0.008422  \n",
       "3             0.844278          0.853606         0.010040  \n",
       "4             0.825516          0.838586         0.011597  \n",
       "..                 ...               ...              ...  \n",
       "58            0.844278          0.853981         0.009929  \n",
       "59            0.844278          0.853606         0.010040  \n",
       "60            0.844278          0.853606         0.010040  \n",
       "61            0.846154          0.853981         0.009714  \n",
       "62            0.846154          0.853981         0.009714  \n",
       "\n",
       "[63 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grid_search_results = pd.DataFrame(grid_search.cv_results_)\n",
    "display(grid_search_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 1, 'max_iter': 10000, 'n_jobs': -1, 'penalty': 'l2'}\n"
     ]
    }
   ],
   "source": [
    "grid_search_params = grid_search.best_params_\n",
    "print(grid_search_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got {'C': 1, 'max_iter': 10000, 'n_jobs': -1, 'penalty': 'l2'}.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [14], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m clf \u001b[38;5;241m=\u001b[39m LogisticRegression(grid_search_params)\n\u001b[0;32m----> 2\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_ct, y)\n\u001b[1;32m      3\u001b[0m y_test \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(clf\u001b[38;5;241m.\u001b[39mpredict(test), name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSurvived\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([test_df_passengeId, y_test], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/garchomp/projects/kaggle/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1091\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfit\u001b[39m(\u001b[39mself\u001b[39m, X, y, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m   1063\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1064\u001b[0m \u001b[39m    Fit the model according to the given training data.\u001b[39;00m\n\u001b[1;32m   1065\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1089\u001b[0m \u001b[39m    The SAGA solver supports both float64 and float32 bit arrays.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1091\u001b[0m     solver \u001b[39m=\u001b[39m _check_solver(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msolver, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpenalty, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdual)\n\u001b[1;32m   1093\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC, numbers\u001b[39m.\u001b[39mNumber) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m   1094\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mPenalty term must be positive; got (C=\u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mC)\n",
      "File \u001b[0;32m~/garchomp/projects/kaggle/.venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:55\u001b[0m, in \u001b[0;36m_check_solver\u001b[0;34m(solver, penalty, dual)\u001b[0m\n\u001b[1;32m     53\u001b[0m all_penalties \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39ml1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39melasticnet\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     54\u001b[0m \u001b[39mif\u001b[39;00m penalty \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m all_penalties:\n\u001b[0;32m---> 55\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     56\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mLogistic Regression supports only penalties in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m, got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     57\u001b[0m         \u001b[39m%\u001b[39m (all_penalties, penalty)\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     60\u001b[0m \u001b[39mif\u001b[39;00m solver \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m [\u001b[39m\"\u001b[39m\u001b[39mliblinear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39msaga\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mand\u001b[39;00m penalty \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     61\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     62\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mSolver \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m supports only \u001b[39m\u001b[39m'\u001b[39m\u001b[39ml2\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m'\u001b[39m\u001b[39m penalties, got \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m penalty.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[39m%\u001b[39m (solver, penalty)\n\u001b[1;32m     64\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: Logistic Regression supports only penalties in ['l1', 'l2', 'elasticnet', 'none'], got {'C': 1, 'max_iter': 10000, 'n_jobs': -1, 'penalty': 'l2'}."
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.set_params(**grid_search_params)\n",
    "clf.fit(X_ct, y)\n",
    "y_test = pd.Series(clf.predict(test), name=\"Survived\").astype(int)\n",
    "result = pd.concat([test_df_passengeId, y_test], axis=1)\n",
    "result.to_csv(\"LogReg.csv\", index=False)\n",
    "display(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e458bfbbc2f229a3f2691b8ad57bc6141d31e4b1d7136b8144d77cb706b999f8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
